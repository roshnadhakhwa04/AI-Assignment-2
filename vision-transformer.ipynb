{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13836,"databundleVersionId":1718836,"sourceType":"competition"},{"sourceId":8432770,"sourceType":"datasetVersion","datasetId":5022306}],"dockerImageVersionId":30061,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries and Configurations","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport glob, warnings\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n\nwarnings.filterwarnings('ignore')\nprint('TensorFlow Version ' + tf.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGE_SIZE = 224 \nBATCH_SIZE = 32 \n# 32 * 32 \nEPOCHS = 7\n\nTRAIN_PATH = '/kaggle/input/ied-data/train'\nTEST_PATH = '/kaggle/input/ied-data/test'\nVAL_PATH = '/kaggle/input/ied-data/val'\n\nclasses = {0 : \"cardboard\",\n           1 : \"glass\",\n           4 : \"ied\",\n           5 : \"metal\",\n           4 : \"paper\",\n           5 : \"plastic\",\n           6 : \"trash\"}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentations","metadata":{}},{"cell_type":"code","source":"def data_augment(image):\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n    p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n    \n    # Flips\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    if p_spatial > .75:\n        image = tf.image.transpose(image)\n        \n    # Rotates\n    if p_rotate > .75:\n        image = tf.image.rot90(image, k = 3) # rotate 270ยบ\n    elif p_rotate > .5:\n        image = tf.image.rot90(image, k = 2) # rotate 180ยบ\n    elif p_rotate > .25:\n        image = tf.image.rot90(image, k = 1) # rotate 90ยบ\n        \n    # Pixel-level transforms\n    if p_pixel_1 >= .4:\n        image = tf.image.random_saturation(image, lower = .7, upper = 1.3)\n    if p_pixel_2 >= .4:\n        image = tf.image.random_contrast(image, lower = .8, upper = 1.2)\n    if p_pixel_3 >= .4:\n        image = tf.image.random_brightness(image, max_delta = .1)\n        \n    return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Generator","metadata":{}},{"cell_type":"code","source":"# Define data generators for train, val, and test sets\ntrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    samplewise_center = True,\n    samplewise_std_normalization = True,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    preprocessing_function = data_augment\n)\n\n\ntrain_gen = train_datagen.flow_from_directory(\n        TRAIN_PATH,\n        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n        batch_size=BATCH_SIZE,\n        class_mode='categorical',\n        classes=['cardboard', 'glass','ied', 'metal','paper', 'plastic','trash'])\n\nval_gen = train_datagen.flow_from_directory(\n        VAL_PATH,\n        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n        batch_size=BATCH_SIZE,\n        class_mode='categorical',\n        classes=['cardboard', 'glass','ied', 'metal','paper', 'plastic','trash'])\n\ntest_gen = train_datagen.flow_from_directory(\n        TEST_PATH,\n        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n        batch_size=BATCH_SIZE,\n        class_mode='categorical',\n        classes=['cardboard', 'glass','ied', 'metal','paper', 'plastic','trash'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = [train_gen[0][0][i] for i in range(16)]\nfig, axes = plt.subplots(3, 5, figsize = (10, 10))\n\naxes = axes.flatten()\n\nfor img, ax in zip(images, axes):\n    ax.imshow(img.reshape(IMAGE_SIZE, IMAGE_SIZE, 3))\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the Model","metadata":{}},{"cell_type":"code","source":"!pip install --quiet vit-keras\n\nfrom vit_keras import vit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. ViT B32 Model","metadata":{}},{"cell_type":"code","source":"vit_model = vit.vit_b32(\n        image_size = IMAGE_SIZE,\n        activation = 'sigmoid',\n        pretrained = True,\n        include_top = False,\n        pretrained_top = False,\n        classes = 7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing Attention Maps of Sample Test Image","metadata":{}},{"cell_type":"markdown","source":"## 2. Fine-tuning the Model","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n        vit_model,\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(11, activation = tfa.activations.gelu),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(7, 'softmax')\n    ],\n    name = 'vision_transformer')\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the Model","metadata":{}},{"cell_type":"code","source":"learning_rate = 1e-4\n\noptimizer = tfa.optimizers.RectifiedAdam(learning_rate = learning_rate)\n\nmodel.compile(optimizer = optimizer, \n              loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2), \n              metrics = ['accuracy'])\n\nSTEP_SIZE_TRAIN = train_gen.n // train_gen.batch_size\nSTEP_SIZE_VALID = val_gen.n // val_gen.batch_size\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy',\n                                                 factor = 0.2,\n                                                 patience = 2,\n                                                 verbose = 1,\n                                                 min_delta = 1e-4,\n                                                 min_lr = 1e-6,\n                                                 mode = 'max')\n\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n                                                 min_delta = 1e-4,\n                                                 patience = 5,\n                                                 mode = 'max',\n                                                 restore_best_weights = True,\n                                                 verbose = 1)\n\ncheckpointer = tf.keras.callbacks.ModelCheckpoint(filepath = './model.hdf5',\n                                                  monitor = 'val_accuracy', \n                                                  verbose = 1, \n                                                  save_best_only = True,\n                                                  save_weights_only = True,\n                                                  mode = 'max')\n\ncallbacks = [earlystopping, reduce_lr, checkpointer]\n\nmodel.fit(x = train_gen,\n          steps_per_epoch = STEP_SIZE_TRAIN,\n          validation_data = val_gen,\n          validation_steps = STEP_SIZE_VALID,\n          epochs = EPOCHS,\n          callbacks = callbacks)\n\nmodel.save('model_b_32.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('model_b_32.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Results","metadata":{}},{"cell_type":"code","source":"predicted_classes = np.argmax(model.predict(test_gen, steps = test_gen.n // test_gen.batch_size + 1), axis = 1)\ntrue_classes = test_gen.classes\nclass_labels = list(test_gen.class_indices.keys())  \n\nconfusionmatrix = confusion_matrix(true_classes, predicted_classes)\nplt.figure(figsize = (16, 16))\nsns.heatmap(confusionmatrix, annot = True, cbar = True)\n\nprint(classification_report(true_classes, predicted_classes))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n# Load the model from the downloaded file\n# loaded_model = tf.keras.models.load_model('model16.h5')\ntest_loss, test_acc = model.evaluate(test_gen, verbose=2)\nprint(f\"Test accuracy: {test_acc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}